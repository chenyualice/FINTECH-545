{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e6714071-2ea4-464f-8675-9be4f65c63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68db13bc-7769-4841-9ddc-764f9c7e2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "portfolio = pd.read_csv(\"initial_portfolio.csv\")\n",
    "rf = pd.read_csv(\"rf.csv\")\n",
    "prices = pd.read_csv(\"DailyPrices.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b59f66b-5d0a-4552-bed3-396ac1c2bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Total Portfolio Attribution\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373 -0.035969   0.204731\n",
      "1  Return Attribution  0.244039 -0.039309   0.204731\n",
      "2     Vol Attribution  0.007207 -0.000131   0.007076\n",
      "Initial A Portfolio Attribution\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373 -0.095555   0.136642\n",
      "1  Return Attribution  0.242621 -0.105980   0.136642\n",
      "2     Vol Attribution  0.007056  0.000348   0.007404 \n",
      "\n",
      "Initial B Portfolio Attribution\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373 -0.028626   0.203526\n",
      "1  Return Attribution  0.234259 -0.030733   0.203526\n",
      "2     Vol Attribution  0.006411  0.000442   0.006854 \n",
      "\n",
      "Initial C Portfolio Attribution\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373  0.022337   0.281172\n",
      "1  Return Attribution  0.255627  0.025546   0.281172\n",
      "2     Vol Attribution  0.007230  0.000678   0.007908 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/30/4j0_qfq56ll_fl0_jwr614tc0000gn/T/ipykernel_70828/1658277011.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  out[var] = p2[:, i]\n"
     ]
    }
   ],
   "source": [
    "#part 1\n",
    "#option 1 for risk free rate\n",
    "\n",
    "def return_calculate(prices, method=\"DISCRETE\", date_column=\"Date\"):\n",
    "\n",
    "    vars = prices.columns.tolist()\n",
    "    vars = [v for v in vars if v != date_column]\n",
    "    \n",
    "    p = prices[vars].values\n",
    "    n = p.shape[0]\n",
    "    m = p.shape[1]\n",
    "    p2 = np.zeros((n-1, m))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        for j in range(m):\n",
    "            p2[i, j] = p[i+1, j] / p[i, j]\n",
    "    \n",
    "    if method.upper() == \"DISCRETE\":\n",
    "        p2 = p2 - 1.0\n",
    "    elif method.upper() == \"LOG\":\n",
    "        p2 = np.log(p2)\n",
    "\n",
    "    dates = prices[date_column].iloc[1:n].reset_index(drop=True)\n",
    "    out = pd.DataFrame({date_column: dates})\n",
    "    \n",
    "    for i, var in enumerate(vars):\n",
    "        out[var] = p2[:, i]\n",
    "    \n",
    "    return out\n",
    "\n",
    "def expost_factor(w, realized_returns, realized_spy, betas): # realized_returns, realized_spy\n",
    "\n",
    "    _realized_spy = realized_spy.copy()\n",
    "    stocks = realized_returns.columns.tolist()\n",
    "    factors = realized_spy.columns.tolist()\n",
    "    \n",
    "    n = len(realized_returns)\n",
    "    m = len(stocks)\n",
    "    \n",
    "    p_return = np.zeros(n)\n",
    "    resid_return = np.zeros(n)\n",
    "    weights = np.zeros((n, len(w)))\n",
    "    factor_weights = np.zeros((n, len(factors)))\n",
    "    last_w = w.copy()\n",
    "    \n",
    "    mat_returns = realized_returns[stocks].values\n",
    "    ff_returns = realized_spy[factors].values\n",
    "\n",
    "    resid_individual = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            resid_individual[i, j] = mat_returns[i, j] - betas[j] * ff_returns[i, 0]\n",
    "    \n",
    "    for i in range(n):\n",
    "        weights[i, :] = last_w\n",
    "        for f in range(len(factors)):\n",
    "            factor_weights[i, f] = np.sum(betas * last_w)\n",
    "        last_w = last_w * (1.0 + mat_returns[i, :])\n",
    "        p_r = np.sum(last_w)\n",
    "        last_w = last_w / p_r\n",
    "        p_return[i] = p_r - 1\n",
    "        resid_return[i] = (p_r - 1) - np.dot(factor_weights[i, :], ff_returns[i, :])\n",
    "    \n",
    "    _realized_spy['Alpha'] = resid_return\n",
    "    _realized_spy['Portfolio'] = p_return\n",
    "    \n",
    "    total_ret = np.exp(np.sum(np.log(p_return + 1))) - 1\n",
    "    \n",
    "    k = np.log(total_ret + 1) / total_ret if total_ret != 0 else 1.0\n",
    "    \n",
    "    carino_k = np.zeros_like(p_return)\n",
    "    for i in range(len(p_return)):\n",
    "        if p_return[i] != 0:\n",
    "            carino_k[i] = np.log(1.0 + p_return[i]) / p_return[i] / k\n",
    "        else:\n",
    "            carino_k[i] = 1.0 / k\n",
    "    attrib = pd.DataFrame(ff_returns * factor_weights * carino_k.reshape(-1, 1), columns=factors)\n",
    "    attrib['Alpha'] = resid_return * carino_k\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            resid_individual[i, j] *= weights[i, j]\n",
    "    attribution = pd.DataFrame({'Value': ['TotalReturn', 'Return Attribution']})\n",
    "    new_factors = factors + ['Alpha']\n",
    "    for s in new_factors + ['Portfolio']:\n",
    "        tr = np.exp(np.sum(np.log(_realized_spy[s] + 1))) - 1\n",
    "        atr = tr if s == 'Portfolio' else np.sum(attrib[s])\n",
    "        attribution[s] = [tr, atr]\n",
    "    Y = np.hstack((ff_returns * factor_weights, resid_return.reshape(-1, 1)))\n",
    "    X = np.column_stack((np.ones(len(p_return)), p_return))\n",
    "    B = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "    B = B[1, :]\n",
    "    cSD = B * np.std(p_return)\n",
    "    vol_attrib = pd.DataFrame({'Value': ['Vol Attribution']})\n",
    "    portfolio_vol = 0\n",
    "    for i, factor in enumerate(new_factors):\n",
    "        portfolio_vol += cSD[i]\n",
    "        vol_attrib[factor] = [cSD[i]]\n",
    "    vol_attrib['Portfolio'] = [portfolio_vol]\n",
    "    attribution = pd.concat([attribution, vol_attrib], ignore_index=True)\n",
    "    return {\n",
    "        'attribution': attribution,\n",
    "        'weights': weights,\n",
    "        'factor_weights': factor_weights,\n",
    "        'resid_individual': resid_individual,\n",
    "        'resid_return': resid_return,\n",
    "        'carino_k': carino_k\n",
    "    }\n",
    "\n",
    "def run_attribution(realized_returns, realized_spy, last_date, start_prices, portfolio, betas, portfolio_type=\"Initial\"):\n",
    "\n",
    "    stocks = portfolio['Symbol'].tolist()\n",
    "    \n",
    "    holdings = portfolio['Holding'].values\n",
    "    start_price_values = start_prices.values[0] \n",
    "    \n",
    "    t_value = np.sum(start_price_values * holdings)\n",
    "    w = (start_price_values * holdings) / t_value\n",
    "    \n",
    "    attrib = expost_factor(w, realized_returns, realized_spy, betas)\n",
    "    print(f\"\\n{portfolio_type} Total Portfolio Attribution\")\n",
    "    print(attrib['attribution'])\n",
    "    \n",
    "    all_results = {\n",
    "        'Total': attrib['attribution']\n",
    "    }\n",
    "    \n",
    "    portfolios = sorted(list(set(portfolio['Portfolio'])))\n",
    "    for p in portfolios:\n",
    "        stocks_mask = portfolio['Portfolio'] == p\n",
    "        p_stocks = portfolio.loc[stocks_mask, 'Symbol'].tolist()\n",
    "        \n",
    "        stock_returns = realized_returns[p_stocks]\n",
    "        p_start_prices = start_prices[p_stocks].values[0]\n",
    "        \n",
    "        p_holdings = portfolio.loc[stocks_mask, 'Holding'].values\n",
    "        \n",
    "        p_t_value = np.sum(p_start_prices * p_holdings)\n",
    "        p_w = (p_start_prices * p_holdings) / p_t_value\n",
    "        \n",
    "        p_betas = np.array([betas[stocks.index(s)] for s in p_stocks])\n",
    "\n",
    "        p_attrib = expost_factor(p_w, stock_returns, realized_spy, p_betas)\n",
    "        print(f\"{portfolio_type} {p} Portfolio Attribution\")\n",
    "        print(p_attrib['attribution'], '\\n')\n",
    "        \n",
    "        all_results[p] = p_attrib['attribution']\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def calculate_expected_returns(betas, avg_mkt_return, avg_rf):\n",
    "\n",
    "    return avg_rf + betas * (avg_mkt_return - avg_rf)\n",
    "\n",
    "def calculate_covariance_matrix(returns, betas, market_returns):\n",
    "\n",
    "    n = len(betas)\n",
    "    market_var = np.var(market_returns)\n",
    "    \n",
    "    resid = np.zeros((len(returns), n))\n",
    "    for i in range(n):\n",
    "        resid[:, i] = returns.iloc[:, i] - betas[i] * market_returns\n",
    "    \n",
    "    resid_var = np.var(resid, axis=0)\n",
    "\n",
    "    cov_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                cov_matrix[i, j] = betas[i]**2 * market_var + resid_var[i]\n",
    "            else:\n",
    "                cov_matrix[i, j] = betas[i] * betas[j] * market_var\n",
    "    \n",
    "    return cov_matrix\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "rf['Date'] = pd.to_datetime(rf['Date'])\n",
    "returns = return_calculate(prices)  # Using our own return_calculate function\n",
    "returns = pd.merge(returns, rf, on='Date', how='left').ffill()\n",
    "\n",
    "# Estimate Betas using OLS\n",
    "stocks = portfolio['Symbol'].tolist()\n",
    "betas_dict = {}\n",
    "sigma_epsilons = {}\n",
    "\n",
    "for stock in stocks:\n",
    "    subset = returns[returns['Date'] < datetime(2023, 12, 31)]\n",
    "    Y = subset[stock] - subset['rf']\n",
    "    X = subset['SPY'] - subset['rf']\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    betas_dict[stock] = list(model.params)[1]  # Index 1 for the slope coefficient\n",
    "    sigma_epsilons[stock] = model.resid.std()\n",
    "\n",
    "# Convert dictionary to list in the same order as stocks\n",
    "betas = np.array([betas_dict[stock] for stock in stocks])\n",
    "\n",
    "# Realized returns post-2024\n",
    "realized_returns = returns[returns['Date'] >= datetime(2024, 1, 1)][stocks]\n",
    "realized_spy = pd.DataFrame({'SPY': returns[returns['Date'] >= datetime(2024, 1, 1)]['SPY']})\n",
    "\n",
    "# Get start prices on last trading day of 2023\n",
    "last_date = returns[returns['Date'] < datetime(2024, 1, 1)]['Date'].max()\n",
    "start_prices = prices[prices['Date'] == last_date][stocks]\n",
    "\n",
    "# Run initial attribution analysis\n",
    "initial_results = run_attribution(realized_returns, realized_spy, last_date, start_prices, portfolio, betas, \"Initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d00528-b485-492f-88c1-16920ed1bf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a284875-3389-4c67-9423-581171447e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Portfolio A \n",
      "Optimal Sharpe Ratio Portfolio Attribution (Option 1, Unconstrained) — Portfolio A\n",
      "Sharpe Ratio (Daily): 0.1009\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.243260 -0.003874   0.239386\n",
      "1  Return Attribution  0.243260 -0.003874   0.239386\n",
      "2     Vol Attribution  0.007503 -0.000119   0.007384\n",
      "\n",
      "Portfolio B \n",
      "Optimal Sharpe Ratio Portfolio Attribution (Option 1, Unconstrained) — Portfolio B\n",
      "Sharpe Ratio (Daily): 0.1028\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.241983 -0.000997   0.240986\n",
      "1  Return Attribution  0.241983 -0.000997   0.240986\n",
      "2     Vol Attribution  0.007338 -0.000030   0.007308\n",
      "\n",
      "Portfolio C \n",
      "Optimal Sharpe Ratio Portfolio Attribution (Option 1, Unconstrained) — Portfolio C\n",
      "Sharpe Ratio (Daily): 0.1135\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.244346  0.035776   0.280123\n",
      "1  Return Attribution  0.244346  0.035776   0.280123\n",
      "2     Vol Attribution  0.006962  0.001019   0.007982\n"
     ]
    }
   ],
   "source": [
    "#part 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pre-2024 returns for estimating expected returns and covariance\n",
    "pre2024_returns = returns[returns['Date'] < '2024-01-01'].copy()\n",
    "pre2024_rf = rf[rf['Date'] < '2024-01-01']['rf']\n",
    "pre2024_market = pre2024_returns['SPY']\n",
    "\n",
    "expected_market_return = pre2024_market.mean()\n",
    "expected_rf = pre2024_rf.mean()\n",
    "\n",
    "# Loop through each sub-portfolio\n",
    "sub_portfolios = portfolio['Portfolio'].unique()\n",
    "\n",
    "for pf_name in sub_portfolios:\n",
    "    print(f\"\\nPortfolio {pf_name} \")\n",
    "\n",
    "    pf_symbols = portfolio[portfolio['Portfolio'] == pf_name]['Symbol'].tolist()\n",
    "\n",
    "    # Estimate CAPM expected returns (alpha = 0, Option 1)\n",
    "    expected_returns_capm = {}\n",
    "    for symbol in pf_symbols:\n",
    "        beta = betas_dict[symbol]\n",
    "        expected_return = expected_rf + beta * (expected_market_return - expected_rf)\n",
    "        expected_returns_capm[symbol] = expected_return\n",
    "\n",
    "    mu_vec = np.array([expected_returns_capm[s] for s in pf_symbols])\n",
    "    excess_mu = mu_vec - expected_rf\n",
    "    cov_matrix = pre2024_returns[pf_symbols].cov()\n",
    "    Sigma = cov_matrix.values\n",
    "\n",
    "    # Unconstrained maximum Sharpe ratio weights\n",
    "    w_unnormalized = np.linalg.inv(Sigma) @ excess_mu\n",
    "    w_optimal = w_unnormalized / np.sum(w_unnormalized)\n",
    "    weights_dict = dict(zip(pf_symbols, w_optimal))\n",
    "\n",
    "    # Return attribution using Option 1 during 2024\n",
    "    returns_2024 = returns[returns['Date'] >= '2024-01-01']\n",
    "    market_2024 = returns_2024['SPY']\n",
    "\n",
    "    total_sys = 0\n",
    "    total_idio = 0\n",
    "    total_ret = 0\n",
    "\n",
    "    for symbol in pf_symbols:\n",
    "        Ri = returns_2024[symbol]\n",
    "        Rm = market_2024\n",
    "        beta = betas_dict[symbol]\n",
    "\n",
    "        SR = beta * Rm  # Systematic return (Option 1)\n",
    "        IR = Ri - SR\n",
    "\n",
    "        weight = w_optimal[pf_symbols.index(symbol)]\n",
    "        total_ret += weight * Ri.sum()\n",
    "        total_sys += weight * SR.sum()\n",
    "        total_idio += weight * IR.sum()\n",
    "\n",
    "    # Daily Sharpe ratio calculation\n",
    "    returns_matrix = returns_2024[pf_symbols].values\n",
    "    weights_vec = np.array([weights_dict[s] for s in pf_symbols])\n",
    "    portfolio_daily_returns = returns_matrix @ weights_vec\n",
    "\n",
    "    daily_mean_return = portfolio_daily_returns.mean()\n",
    "    portfolio_std = portfolio_daily_returns.std()\n",
    "    sharpe_ratio = (daily_mean_return - expected_rf) / portfolio_std  # daily Sharpe\n",
    "\n",
    "    # Attribution table (daily vol-based)\n",
    "    attribution_table = pd.DataFrame({\n",
    "        'Value': ['TotalReturn', 'Return Attribution', 'Vol Attribution'],\n",
    "        'SPY': [total_sys, total_sys, portfolio_std * (total_sys / total_ret)],\n",
    "        'Alpha': [total_idio, total_idio, portfolio_std * (total_idio / total_ret)],\n",
    "        'Portfolio': [total_ret, total_ret, portfolio_std]\n",
    "    })\n",
    "\n",
    "    print(f\"Optimal Sharpe Ratio Portfolio Attribution (Option 1, Unconstrained) — Portfolio {pf_name}\")\n",
    "    print(f\"Sharpe Ratio (Daily): {sharpe_ratio:.4f}\")\n",
    "    print(attribution_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5ae0b-9a43-4aac-b0d1-f39e4f46a49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18ff2018-cd51-4512-9c6a-866c7e8fa142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Fit</th>\n",
       "      <th>Params</th>\n",
       "      <th>AIC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WFC</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 5.001478131472827, 'scale': 0.013690793...</td>\n",
       "      <td>-1322.469459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETN</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 3.9195527205129665, 'scale': 0.01206865...</td>\n",
       "      <td>-1355.429155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 5.954095317120535, 'scale': 0.016917840...</td>\n",
       "      <td>-1234.173899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QCOM</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 5.228936791555922, 'scale': 0.015619414...</td>\n",
       "      <td>-1261.503045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LMT</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 3.7066212344259615, 'scale': 0.00739510...</td>\n",
       "      <td>-1591.359489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 7.822557002480616, 'scale': 0.013629964...</td>\n",
       "      <td>-1363.007912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEP</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 5.81887209927161, 'scale': 0.0076155867...</td>\n",
       "      <td>-1629.584382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 5.697500393199872, 'scale': 0.010327721...</td>\n",
       "      <td>-1475.924947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PANW</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 3.3187531696217416, 'scale': 0.01555146...</td>\n",
       "      <td>-1203.908904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLK</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>{'df': 7.928452704245332, 'scale': 0.012039563...</td>\n",
       "      <td>-1425.693129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Best Fit                                             Params  \\\n",
       "Symbol                                                                     \n",
       "WFC     Generalized T  {'df': 5.001478131472827, 'scale': 0.013690793...   \n",
       "ETN     Generalized T  {'df': 3.9195527205129665, 'scale': 0.01206865...   \n",
       "AMZN    Generalized T  {'df': 5.954095317120535, 'scale': 0.016917840...   \n",
       "QCOM    Generalized T  {'df': 5.228936791555922, 'scale': 0.015619414...   \n",
       "LMT     Generalized T  {'df': 3.7066212344259615, 'scale': 0.00739510...   \n",
       "...               ...                                                ...   \n",
       "MSFT    Generalized T  {'df': 7.822557002480616, 'scale': 0.013629964...   \n",
       "PEP     Generalized T  {'df': 5.81887209927161, 'scale': 0.0076155867...   \n",
       "CB      Generalized T  {'df': 5.697500393199872, 'scale': 0.010327721...   \n",
       "PANW    Generalized T  {'df': 3.3187531696217416, 'scale': 0.01555146...   \n",
       "BLK     Generalized T  {'df': 7.928452704245332, 'scale': 0.012039563...   \n",
       "\n",
       "                AIC  \n",
       "Symbol               \n",
       "WFC    -1322.469459  \n",
       "ETN    -1355.429155  \n",
       "AMZN   -1234.173899  \n",
       "QCOM   -1261.503045  \n",
       "LMT    -1591.359489  \n",
       "...             ...  \n",
       "MSFT   -1363.007912  \n",
       "PEP    -1629.584382  \n",
       "CB     -1475.924947  \n",
       "PANW   -1203.908904  \n",
       "BLK    -1425.693129  \n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Fit Distribution Summary (Stock Count):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distribution</th>\n",
       "      <th>Stock Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generalized T</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIG</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Distribution  Stock Count\n",
       "0  Generalized T           97\n",
       "1            NIG            2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#part 4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, skewnorm, norminvgauss, t as t_dist\n",
    "\n",
    "# Filter returns before 2024\n",
    "pre2024_returns = returns[returns['Date'] < '2024-01-01'].copy()\n",
    "pre2024_returns.set_index('Date', inplace=True)\n",
    "\n",
    "fit_results = {}\n",
    "\n",
    "for symbol in stocks:\n",
    "    data = pre2024_returns[symbol].dropna().values\n",
    "    data = data - data.mean()  # Set mean ≈ 0\n",
    "\n",
    "    dist_names = ['Normal', 'Generalized T', 'NIG', 'Skew Normal']\n",
    "    aic_scores = {}\n",
    "    fitted_params = {}\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    # Normal\n",
    "    try:\n",
    "        mu, std = norm.fit(data)\n",
    "        ll = np.sum(norm.logpdf(data, loc=0, scale=std))  # Fix mean = 0\n",
    "        k = 1\n",
    "        aic_scores['Normal'] = 2 * k - 2 * ll\n",
    "        fitted_params['Normal'] = {'std': std}\n",
    "    except:\n",
    "        aic_scores['Normal'] = np.inf\n",
    "        fitted_params['Normal'] = None\n",
    "\n",
    "    # Generalized T (using scipy.stats.t)\n",
    "    try:\n",
    "        df, loc, scale = t_dist.fit(data, floc=0)\n",
    "        ll = np.sum(t_dist.logpdf(data, df, loc=0, scale=scale))\n",
    "        k = 2\n",
    "        aic_scores['Generalized T'] = 2 * k - 2 * ll\n",
    "        fitted_params['Generalized T'] = {'df': df, 'scale': scale}\n",
    "    except:\n",
    "        aic_scores['Generalized T'] = np.inf\n",
    "        fitted_params['Generalized T'] = None\n",
    "\n",
    "    # NIG\n",
    "    try:\n",
    "        a, b, loc, scale = norminvgauss.fit(data, floc=0)\n",
    "        ll = np.sum(norminvgauss.logpdf(data, a, b, loc=0, scale=scale))\n",
    "        k = 3\n",
    "        aic_scores['NIG'] = 2 * k - 2 * ll\n",
    "        fitted_params['NIG'] = {'a': a, 'b': b, 'scale': scale}\n",
    "    except:\n",
    "        aic_scores['NIG'] = np.inf\n",
    "        fitted_params['NIG'] = None\n",
    "\n",
    "    # Skew Normal\n",
    "    try:\n",
    "        a, loc, scale = skewnorm.fit(data, floc=0)\n",
    "        ll = np.sum(skewnorm.logpdf(data, a, loc=0, scale=scale))\n",
    "        k = 2\n",
    "        aic_scores['Skew Normal'] = 2 * k - 2 * ll\n",
    "        fitted_params['Skew Normal'] = {'a': a, 'scale': scale}\n",
    "    except:\n",
    "        aic_scores['Skew Normal'] = np.inf\n",
    "        fitted_params['Skew Normal'] = None\n",
    "\n",
    "    # Determine best model by AIC\n",
    "    best_model = min(aic_scores, key=aic_scores.get)\n",
    "    fit_results[symbol] = {\n",
    "        'Best Fit': best_model,\n",
    "        'Params': fitted_params[best_model],\n",
    "        'AIC': aic_scores[best_model]\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "fit_df = pd.DataFrame(fit_results).T\n",
    "fit_df.index.name = 'Symbol'\n",
    "\n",
    "# Display fit results\n",
    "display(fit_df)\n",
    "\n",
    "# Count how many stocks selected each best-fit model\n",
    "fit_summary = fit_df['Best Fit'].value_counts().rename_axis('Distribution').reset_index(name='Stock Count')\n",
    "\n",
    "print(\"\\nBest Fit Distribution Summary (Stock Count):\")\n",
    "display(fit_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26f96639-0584-4375-a648-3d82affaaa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Portfolio 1-day VaR and ES (Copula vs MVN):\n",
      "Portfolio  VaR_95 (Copula $)  ES_95 (Copula $)  VaR_95 (MVN $)  ES_95 (MVN $)  VaR_95 (Copula %)  ES_95 (Copula %)  VaR_95 (MVN %)  ES_95 (MVN %)\n",
      "        A            4174.03           6001.28         4112.91        5183.07               1.41              2.03            1.39           1.75\n",
      "        B            3836.07           5311.78         3590.75        4531.93               1.37              1.89            1.28           1.61\n",
      "        C            4120.11           5572.58         3470.12        4353.63               1.54              2.08            1.30           1.63\n",
      "    Total           11741.71          16453.37        10805.91       13826.27               1.39              1.95            1.28           1.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import norm, t as t_dist, skewnorm, norminvgauss, multivariate_normal\n",
    "\n",
    "# Prepare price dataframe\n",
    "prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "price_df = prices.set_index('Date')\n",
    "\n",
    "# Extract symbol list from portfolio\n",
    "stocks = portfolio['Symbol'].unique().tolist()\n",
    "\n",
    "# Compute daily log returns before 2024\n",
    "log_returns = np.log(price_df[stocks] / price_df[stocks].shift(1)).dropna()\n",
    "pre2024_returns = log_returns[log_returns.index < '2024-01-01']\n",
    "\n",
    "# Convert historical returns to CDF values (marginals) using best-fit distribution\n",
    "u_data = pd.DataFrame(columns=stocks)\n",
    "\n",
    "for stock in stocks:\n",
    "    dist_name = fit_df.loc[stock, 'Best Fit']\n",
    "    params = fit_df.loc[stock, 'Params']\n",
    "    x = pre2024_returns[stock].dropna()\n",
    "\n",
    "    if dist_name == 'Normal':\n",
    "        u_data[stock] = norm.cdf(x, loc=0, scale=params['std'])\n",
    "    elif dist_name == 'Generalized T':\n",
    "        u_data[stock] = t_dist.cdf(x, df=params['df'], loc=0, scale=params['scale'])\n",
    "    elif dist_name == 'Skew Normal':\n",
    "        u_data[stock] = skewnorm.cdf(x, a=params['a'], loc=0, scale=params['scale'])\n",
    "    elif dist_name == 'NIG':\n",
    "        u_data[stock] = norminvgauss.cdf(x, a=params['a'], b=params['b'], loc=0, scale=params['scale'])\n",
    "\n",
    "# Fit Gaussian copula using correlation of uniform marginals\n",
    "R = u_data.corr()\n",
    "n_sim = 1000\n",
    "copula = multivariate_normal(mean=np.zeros(len(stocks)), cov=R)\n",
    "sim_u = pd.DataFrame(norm.cdf(copula.rvs(size=n_sim)), columns=stocks)\n",
    "\n",
    "# Invert the CDF to obtain simulated returns from uniform samples\n",
    "simulated_returns = pd.DataFrame(columns=stocks)\n",
    "\n",
    "for stock in stocks:\n",
    "    dist_name = fit_df.loc[stock, 'Best Fit']\n",
    "    params = fit_df.loc[stock, 'Params']\n",
    "    u = np.clip(sim_u[stock], 1e-6, 1 - 1e-6)\n",
    "\n",
    "    if dist_name == 'Normal':\n",
    "        simulated_returns[stock] = norm.ppf(u, loc=0, scale=params['std'])\n",
    "    elif dist_name == 'Generalized T':\n",
    "        simulated_returns[stock] = t_dist.ppf(u, df=params['df'], loc=0, scale=params['scale'])\n",
    "    elif dist_name == 'Skew Normal':\n",
    "        simulated_returns[stock] = skewnorm.ppf(u, a=params['a'], loc=0, scale=params['scale'])\n",
    "    elif dist_name == 'NIG':\n",
    "        simulated_returns[stock] = norminvgauss.ppf(u, a=params['a'], b=params['b'], loc=0, scale=params['scale'])\n",
    "\n",
    "# Simulate returns from multivariate normal (MVN) as baseline\n",
    "mean_vec = pre2024_returns[stocks].mean().values\n",
    "cov_mat = pre2024_returns[stocks].cov().values\n",
    "mvn_samples = np.random.multivariate_normal(mean_vec, cov_mat, size=n_sim)\n",
    "mvn_df = pd.DataFrame(mvn_samples, columns=stocks)\n",
    "\n",
    "# Compute 1-day VaR and ES for each portfolio and for the total portfolio\n",
    "portfolio_results = []\n",
    "\n",
    "for name in ['A', 'B', 'C', 'Total']:\n",
    "    if name != 'Total':\n",
    "        sub = portfolio[portfolio['Portfolio'] == name].set_index('Symbol')\n",
    "    else:\n",
    "        sub = portfolio.set_index('Symbol')\n",
    "\n",
    "    holdings = sub['Holding']\n",
    "    prices_at_start = price_df.loc['2023-12-29', holdings.index]\n",
    "    values = holdings * prices_at_start\n",
    "    weights = values / values.sum()\n",
    "    valid_stocks = [s for s in stocks if s in weights.index]\n",
    "    w_vec = weights.reindex(valid_stocks).fillna(0).values\n",
    "\n",
    "    sim_port_returns = simulated_returns[valid_stocks].dot(w_vec)\n",
    "    mvn_port_returns = mvn_df[valid_stocks].dot(w_vec)\n",
    "\n",
    "    VaR_95_cop = np.percentile(sim_port_returns, 5)\n",
    "    ES_95_cop = sim_port_returns[sim_port_returns <= VaR_95_cop].mean()\n",
    "    VaR_95_mvn = np.percentile(mvn_port_returns, 5)\n",
    "    ES_95_mvn = mvn_port_returns[mvn_port_returns <= VaR_95_mvn].mean()\n",
    "\n",
    "    port_value = values.sum()\n",
    "    portfolio_results.append({\n",
    "        'Portfolio': name,\n",
    "        'VaR_95 (Copula $)': round(abs(VaR_95_cop * port_value), 2),\n",
    "        'ES_95 (Copula $)': round(abs(ES_95_cop * port_value), 2),\n",
    "        'VaR_95 (MVN $)': round(abs(VaR_95_mvn * port_value), 2),\n",
    "        'ES_95 (MVN $)': round(abs(ES_95_mvn * port_value), 2),\n",
    "        'VaR_95 (Copula %)': round(abs(VaR_95_cop) * 100, 2),\n",
    "        'ES_95 (Copula %)': round(abs(ES_95_cop) * 100, 2),\n",
    "        'VaR_95 (MVN %)': round(abs(VaR_95_mvn) * 100, 2),\n",
    "        'ES_95 (MVN %)': round(abs(ES_95_mvn) * 100, 2)\n",
    "    })\n",
    "\n",
    "# Output results\n",
    "var_es_summary = pd.DataFrame(portfolio_results)\n",
    "print(\"\\nPortfolio 1-day VaR and ES (Copula vs MVN):\")\n",
    "print(var_es_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eee959c3-acf7-41aa-9b7c-5b1a33687bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Parity for Portfolio A \n",
      "Risk Parity Weights (based on ES):\n",
      "WFC: 0.0303\n",
      "ETN: 0.0303\n",
      "AMZN: 0.0303\n",
      "QCOM: 0.0303\n",
      "LMT: 0.0303\n",
      "KO: 0.0303\n",
      "JNJ: 0.0303\n",
      "ISRG: 0.0303\n",
      "XOM: 0.0303\n",
      "MDT: 0.0303\n",
      "DHR: 0.0303\n",
      "PLD: 0.0303\n",
      "BA: 0.0303\n",
      "PG: 0.0303\n",
      "MRK: 0.0303\n",
      "AMD: 0.0303\n",
      "BX: 0.0303\n",
      "PM: 0.0303\n",
      "SCHW: 0.0303\n",
      "VZ: 0.0303\n",
      "COP: 0.0303\n",
      "ADI: 0.0303\n",
      "BAC: 0.0303\n",
      "NOW: 0.0303\n",
      "TMO: 0.0303\n",
      "CVX: 0.0303\n",
      "ANET: 0.0303\n",
      "NVDA: 0.0303\n",
      "GE: 0.0303\n",
      "GILD: 0.0303\n",
      "MU: 0.0303\n",
      "CMCSA: 0.0303\n",
      "DIS: 0.0303\n",
      "\n",
      "Risk Parity for Portfolio B \n",
      "Risk Parity Weights (based on ES):\n",
      "AXP: 0.0303\n",
      "HON: 0.0303\n",
      "META: 0.0303\n",
      "NFLX: 0.0303\n",
      "PGR: 0.0303\n",
      "LLY: 0.0303\n",
      "JPM: 0.0303\n",
      "VRTX: 0.0303\n",
      "TJX: 0.0303\n",
      "EQIX: 0.0303\n",
      "AAPL: 0.0303\n",
      "FI: 0.0303\n",
      "DE: 0.0303\n",
      "SBUX: 0.0303\n",
      "GOOGL: 0.0303\n",
      "T: 0.0303\n",
      "ABT: 0.0303\n",
      "BMY: 0.0303\n",
      "MS: 0.0303\n",
      "CRM: 0.0303\n",
      "PFE: 0.0303\n",
      "SPGI: 0.0303\n",
      "BRK-B: 0.0303\n",
      "ADBE: 0.0303\n",
      "ACN: 0.0303\n",
      "AMGN: 0.0303\n",
      "LIN: 0.0303\n",
      "V: 0.0303\n",
      "WMT: 0.0303\n",
      "AMAT: 0.0303\n",
      "CAT: 0.0303\n",
      "RTX: 0.0303\n",
      "UNP: 0.0303\n",
      "\n",
      "Risk Parity for Portfolio C \n",
      "Risk Parity Weights (based on ES):\n",
      "IBM: 0.0303\n",
      "TXN: 0.0303\n",
      "ADP: 0.0303\n",
      "GOOG: 0.0303\n",
      "ORCL: 0.0303\n",
      "BSX: 0.0303\n",
      "UNH: 0.0303\n",
      "TMUS: 0.0303\n",
      "SYK: 0.0303\n",
      "GS: 0.0303\n",
      "UBER: 0.0303\n",
      "AVGO: 0.0303\n",
      "MMC: 0.0303\n",
      "CSCO: 0.0303\n",
      "PLTR: 0.0303\n",
      "MA: 0.0303\n",
      "C: 0.0303\n",
      "BKNG: 0.0303\n",
      "MCD: 0.0303\n",
      "LOW: 0.0303\n",
      "HD: 0.0303\n",
      "INTU: 0.0303\n",
      "LRCX: 0.0303\n",
      "KKR: 0.0303\n",
      "COST: 0.0303\n",
      "NEE: 0.0303\n",
      "ABBV: 0.0303\n",
      "TSLA: 0.0303\n",
      "MSFT: 0.0303\n",
      "PEP: 0.0303\n",
      "CB: 0.0303\n",
      "PANW: 0.0303\n",
      "BLK: 0.0303\n"
     ]
    }
   ],
   "source": [
    "#part 5\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def compute_es_per_stock(simulated_returns, alpha=0.05):\n",
    "    es_dict = {}\n",
    "    for symbol in simulated_returns.columns:\n",
    "        data = simulated_returns[symbol].dropna()\n",
    "        var = np.percentile(data, alpha * 100)\n",
    "        es = data[data <= var].mean()\n",
    "        es_dict[symbol] = abs(es)  # Use absolute ES as risk\n",
    "    return es_dict\n",
    "\n",
    "def get_risk_parity_weights(es_dict):\n",
    "    assets = list(es_dict.keys())\n",
    "    es_values = np.array([es_dict[a] for a in assets])\n",
    "\n",
    "    def objective(w):\n",
    "        total_risk = np.dot(w, es_values)\n",
    "        risk_contrib = w * es_values\n",
    "        avg_risk = total_risk / len(w)\n",
    "        return np.sum((risk_contrib - avg_risk) ** 2)\n",
    "\n",
    "    n = len(assets)\n",
    "    w0 = np.ones(n) / n\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = [(0, 1)] * n\n",
    "\n",
    "    result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    weights = result.x\n",
    "    return dict(zip(assets, weights))\n",
    "\n",
    "#  for each sub-portfolio \n",
    "risk_parity_weights = {}\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    print(f\"\\nRisk Parity for Portfolio {name} \")\n",
    "\n",
    "    sub = portfolio[portfolio['Portfolio'] == name].set_index('Symbol')\n",
    "    sub_stocks = sub.index.tolist()\n",
    "    \n",
    "    # calculate stock-level ES\n",
    "    es_dict = compute_es_per_stock(simulated_returns[sub_stocks], alpha=0.05)\n",
    "    \n",
    "    # optimize for risk parity weights\n",
    "    weights = get_risk_parity_weights(es_dict)\n",
    "    risk_parity_weights[name] = weights\n",
    "\n",
    "    # Display result\n",
    "    print(\"Risk Parity Weights (based on ES):\")\n",
    "    for k, v in weights.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6dfe764-bf0c-4dbe-ac3a-67f514853c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attribution using ES-based Risk Parity Portfolios\n",
      "\n",
      "Portfolio A\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373 -0.032882   0.229236\n",
      "1  Return Attribution  0.265584 -0.036348   0.229236\n",
      "2     Vol Attribution  0.007683  0.000433   0.008116 \n",
      "\n",
      "Portfolio B\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373  0.014831   0.255865\n",
      "1  Return Attribution  0.238187  0.017678   0.255865\n",
      "2     Vol Attribution  0.006423  0.000386   0.006809 \n",
      "\n",
      "Portfolio C\n",
      "                Value       SPY     Alpha  Portfolio\n",
      "0         TotalReturn  0.261373  0.096561   0.397244\n",
      "1  Return Attribution  0.287391  0.109853   0.397244\n",
      "2     Vol Attribution  0.007809  0.000980   0.008789 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAttribution using ES-based Risk Parity Portfolios\\n\")\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    print(f\"Portfolio {name}\")\n",
    "\n",
    "    sub_pf = portfolio[portfolio['Portfolio'] == name]\n",
    "    symbols = sub_pf['Symbol'].tolist()\n",
    "    \n",
    "    # Get corresponding betas\n",
    "    pf_betas = np.array([betas_dict[s] for s in symbols])\n",
    "    \n",
    "    # Get 2024 returns\n",
    "    stock_returns = realized_returns[symbols]\n",
    "    \n",
    "    # Get SPY returns\n",
    "    spy_returns = realized_spy.copy()\n",
    "    \n",
    "    # Get risk parity weights\n",
    "    weights_dict = risk_parity_weights[name]\n",
    "    weights_vec = np.array([weights_dict[s] for s in symbols])\n",
    "    \n",
    "    # Run attribution\n",
    "    attrib_result = expost_factor(weights_vec, stock_returns, spy_returns, pf_betas)\n",
    "    print(attrib_result['attribution'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfc4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf063f-7f83-4d87-9ea0-762b03c7b1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
